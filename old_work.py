# -*- coding: utf-8 -*-
"""Old_Work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZWqF54jWi5DOLhmXnu9YwUbJx-hNW7an
"""

# Commented out IPython magic to ensure Python compatibility.

import numpy as np
import pandas as pd
from scipy.optimize import minimize
import matplotlib
# %matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

# This notebook uses global.1751_2014.csv, co2_annmean_mlo.csv , btc.csv, and eth.csv.

"""## Q1:  CO$_2$ in the atmosphere (10 points)

Let us examine the Mona Loa atmospheric composition timeseries.
Dr. Pieter Tans, "Trends in Atmospheric Carbon Dioxide."  NOAA/GML (gml.noaa.gov/ccgg/trends/) and Dr. Ralph Keeling, Scripps Institution of Oceanography (scrippsco2.ucsd.edu/).
The data product we're after is the annual mean CO2 from 1960 to 2021:
https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv
"""

co = pd.read_csv("/co2_annmean_mlo.csv", comment="#")
co["Mean"]= co["mean"]

co.plot("year", "Mean")
plt.ylabel("atmospheric CO$_2$ at Mona Loa\n ppm in dry air")

co.index = co.year
co.index
co.loc[2010:2021]

"""**Question 1**  find and plot simple least-squares fit lines for the mean atmospheric
CO$_2$ data for the entire timeseries and for the first half ( 1959-1990) and second half(1991-2021).
For this problem, write a function to evaluate the sums of the squared error, and optimize this function.  Don't use `linregress`, but you can use `optimize` or `minimize`.  
"""

def line_mse(x,y):
    if len(x)!=len(y):
        raise ValueError('arrays are of different lengths')
    def mse(a):
        estimate = a[0]*x + a[1]
        return (np.mean((y - estimate) ** 2))
    return minimize(mse,x0=[0,0],method="BFGS")

line_mse(co['year'],
         co['Mean']).x,line_mse(co['year'].loc[1959:1990],
                                co['Mean'].loc[1959:1990]).x,line_mse(
                                    co['year'].loc[1991:2021],
                                    co['Mean'].loc[1991:2021]).x

plt.figure(figsize=(8, 18))
plt.subplot(4,2,1)
plt.title("Year vs. Mean")
plt.scatter(co['year'], co['Mean'])
xlims=np.array([1959,2021])
plt.plot(xlims,1.61393580e+00*xlims-2.85439367e+03,color="red");
plt.subplot(4,2,2)
plt.title("Year(1959-1990) vs. Mean")
plt.scatter(co['year'].loc[1959:1990], co['Mean'].loc[1959:1990])
xlims=np.array([1959,1990])
plt.plot(xlims,1.25852650e+00*xlims-2.15250994e+03,color="red");
plt.subplot(4,2,3)
plt.title("Year(1991-2021) vs. Mean")
plt.scatter(co['year'].loc[1991:2021], co['Mean'].loc[1991:2021])
xlims=np.array([1991,2021])
plt.plot(xlims,2.05703267e+00*xlims-3.74337819e+03,color="red");

"""**Question 2**  What do the slopes mean?  Estimate the year the atmosphere will
pass the 560 ppm "doubling of preindustrial atmospheric carbon" landmark.

Remember the calculus operation to find slopes is differentiation, but
this is data sampled at regular intervals, not a continuous function, so
we can approximate the derivative with finite differences:

`np.diff(y) / np.diff(x)` gives us n-1 estimates of dy/dx.

(Why is it n-1?)

It's n-1 b/c **xxx**
"""

(560+2.85439367e+03)/1.61393580e+00, (560+3.74337819e+03)/2.05703267e+00

"""There isn't a big mystery about where this excess carbon dioxide  cisoming from, it's human economic activity tied to the production of energy and construction materials.

Boden, T.A., Marland, G., and Andres, R.J. (2017). Global, Regional, and National Fossil-Fuel CO2 Emissions. Carbon Dioxide Information Analysis Center, Oak Ridge National Laboratory, U.S. Department of Energy, Oak Ridge, Tenn., U.S.A. doi 10.3334/CDIAC/00001_V2017.
   
"""

#Slopes Mean the rate of change for the mean per year



"""**Question 3**
Find a simple-least-squares regression line for the relationship between year-over-year
atmospheric CO$_2$ concentration difference and the estimated world production of CO$_2$.
Interpret the slope coefficient.
"""

emissions = pd.read_csv("/global.1751_2014.csv")
(emissions.loc[1959:1990]).head(50)
emissions.index = emissions.Year
emissions.index
emissions.loc[1959:1990]
#emissions.loc[1959:1990].plot("Year", "Total")
xlims=np.array([312,355])
plt.plot(co["Mean"].loc[1959:1990], emissions["Total"].loc[1959:1990])
plt.ylabel("Global Carbon (MMT), estimated")
plt.xlabel("atmospheric CO$_2$ at Mona Loa\n ppm in dry air")
plt.plot(xlims,91.29882537*xlims-25954.78905036,color="red");

line_mse(co["Mean"].loc[1959:1990],
         emissions["Total"].loc[1959:1990]).x

"""## Q2. Set card game (20 points)

<img src=https://upload.wikimedia.org/wikipedia/commons/8/8f/Set-game-cards.png>

There is a devilishly abstract card game called SET.

The game is played with a particular deck of 81 cards.  

Each card has four characteristics, shape, number, shading pattern and color.  

Each of the characteristics takes 3 values: (colors: red, green, purple;
shapes: oval, diamond, tilde; numbers: 1, 2, 3; patterns: solid, hollow, shaded).  The 81 cards have one of each of every possible of the  $3^4$ combinations.

Wikipedia's summary of the game rules https://en.wikipedia.org/wiki/Set_(card_game) :
    
    A 'valid set' consists of three cards satisfying all of these conditions:

    They all have the same number or have three different numbers.
    They all have the same shape or have three different shapes.
    They all have the same shading or have three different shadings.
    They all have the same color or have three different colors.
    
These requirements make the cards of a valid set have 1, 2, or 3 characteristics
(shape, number, shading, and color) the same and the remaining characteristics
all different.  (All four characteristics cannot be the same because the deck has
only one of each card.)

You can also find a description of the rules of the game in a New York Times puzzle column from 2016:
https://www.nytimes.com/2016/08/22/crosswords/the-problem-with-set.html  Antonick, Gary. The Problem with SETÂ®.  New York Times  2016-08-22

Our tasks are 1) writing functions to check whether three cards form a valid set 2) checking all possible subsets of three cards in a larger hand and 3) estimating the fraction of the time a random hand of a given size has a valid set.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import itertools
import random
from pandas.core.reshape.util import cartesian_product

"""Hints: To show you some useful funcitons, let us play with the following list:
    
"""

counting = ["one", "two", "three", "four", "five", "six"]

# Potentially useful tool 1:  random.sample
# (without replacement)  grab one card:

sample =  random.sample(list(counting), 1)
sample3 =  random.sample(list(counting), 3)

sample, sample3

# Potentially useful tool 2:  itertools.combinations( list, n )
# returns every possible combination of n items drawn from
# list without replacement.
print(counting)
list(itertools.combinations(counting, 2))

# construct a dataframe called universe that contains
# the entire deck of cards.

number =  [  1,   2,   3]  #number of symbols
shape =   ["o", "d", "s"]  # shape of symbols
shading = [".", ",", "#"]  # shading of symbols
color =   ["R", "G", "P"]  # color of symbols

sampleset = np.array ( [
    [ "1", "d", ".", "G"],
    [ "2", "o", ",", "P"],
    [ "3", "s", "#", "R"] ] ) # This is the encoding for
# the set of three cards in the stock image above.

universe = np.vstack(cartesian_product([number, shape, shading, color])).T  # This is a numpy array
universe

universe.shape  # 81 cards x 4 properties

# grab 3 cards:
sample3 = random.sample(list(universe), 3)
sample3

# grab 9 cards:

ninecards = random.sample(list(universe), 9)  # returns a list of arrays..
ninecards



list(itertools.combinations(ninecards, 2))  # here is a list of all possible pairs of rows



"""**Question 1** write a function that tests whether three cards form a "valid set."  This means of the four properties, 1, 2 or 3  must all be the same, and the remaining properties must be all different."""

def test_3cards_set(listofcards):
    '''returns true or false when list of (three) cards has
    all-the-same or all-different characters for all (four)
    characters.'''
    assert len(listofcards) == 3 # ABORT if input not as expected
    ans = ((((listofcards[0][0] == listofcards[1][0]) and (listofcards[0][0] == listofcards[2][0]) and (listofcards[1][0] == listofcards[2][0]))
    or ((listofcards[0][0] != listofcards[1][0]) and (listofcards[0][0] != listofcards[2][0]) and (listofcards[1][0] != listofcards[2][0])))
    and
    (((listofcards[0][1] == listofcards[1][1]) and (listofcards[0][1] == listofcards[2][1]) and (listofcards[1][1] == listofcards[2][1]))
    or ((listofcards[0][1] != listofcards[1][1]) and (listofcards[0][1] != listofcards[2][1]) and (listofcards[1][1] != listofcards[2][1])))
    and
    (((listofcards[0][2] == listofcards[1][2]) and (listofcards[0][2] == listofcards[2][2]) and (listofcards[1][2] == listofcards[2][2]))
    or ((listofcards[0][2] != listofcards[1][2]) and (listofcards[0][2] != listofcards[2][2]) and (listofcards[1][2] != listofcards[2][2])))
    and
    (((listofcards[0][3] == listofcards[1][3]) and (listofcards[0][3] == listofcards[2][3]) and (listofcards[1][3] == listofcards[2][3]))
    or ((listofcards[0][3] != listofcards[1][3]) and (listofcards[0][3] != listofcards[2][3]) and (listofcards[1][3] != listofcards[2][3]))))

    return ans



# Your code should be able to give the right answers for these:
set1 = [np.array(['3', 's', ',', 'G']),
        np.array(['3', 'o', '#', 'R']),
        np.array(['1', 'o', '.', 'G'])]
# two numbers, two shapes, three shadings, two colors, no valid set.

set2 = [np.array(['3', 'o', ',', 'G']),
        np.array(['3', 'o', '.', 'R']),
        np.array(['3', 'o', '#', 'P'])]
# one number, one shape, three shadings, three colors, valid set.

set3 = [np.array(['3', 'o', ',', 'G']),
        np.array(['2', 's', '.', 'R']),
        np.array(['1', 'd', '#', 'P'])]
# three numbers, three shapes, three shadings, three colors, valid set.
set4 = [np.array(['1', 'o', ',', 'G']),
        np.array(['1', 's', '.', 'R']),
        np.array(['1', 'd', '#', 'P'])]
# one number, three shapes, three shadings, three colors, valid set.

"""This should return True, True, True after you have written `test_3cards_set()`"""

(test_3cards_set(set1) == 0,
test_3cards_set(set2) == 1,
test_3cards_set(set3) == 1,
test_3cards_set(set4) == 1)

print (len(set1))

"""**Question 2**  Now that you have a function that tests three cards for whether make a valid set,
write a function `test_morecards_set(cards)`  that tests all possible subsets of three cards in `cards` and returns the number of valid sets.
"""

set8 = [np.array(['3', 's', ',', 'G']),
        np.array(['3', 'o', '#', 'R']),
        np.array(['1', 'o', '.', 'G']),
        np.array(['1', 'o', ',', 'G']),
        np.array(['1', 's', '.', 'R'])]

import itertools

def findsubsets(s, n):
    return list(itertools.combinations(s, n))

# Driver Code
s = {1, 2, 3}
n = 2

len(findsubsets(set8, 3))
#(findsubsets(set5, 3)[-1])(findsubsets(set5, 3)[9])
(findsubsets(set8, 3)[0:8])
set8

def test_morecards_set_helper(his,j):
  y=j
  if len(his)>1:
    if (test_3cards_set(his[-1])) == False:
      return test_morecards_set_helper(his[0:len(his)-1], j)
    elif (test_3cards_set(his[-1])) == True:
      return test_morecards_set_helper(his[0:len(his)-1], j+1)
  elif len(his)==1:
    if (test_3cards_set(his[-1])) == False:
      return y
    elif (test_3cards_set(his[-1])) == True:
      return y+1
  return y
def test_morecards_set(cards):
  hi = findsubsets(cards, 3)
  if len(hi) == 1:
    return int(test_3cards_set(cards))
  else:
    return test_morecards_set_helper(hi,0)


#test_3cards_set(findsubsets(set6, 3)[-1])

test_morecards_set(set8)

# We can use the test fixtures we wrote before still..
(test_morecards_set(set1)==0,
 test_morecards_set(set2)==1,
 test_morecards_set(set3)==1)

# But need new test fixtures to verify that subsets are actually
# being checked.
set5 = [np.array(['3', 's', ',', 'G']),
        np.array(['3', 'o', '#', 'R']),
        np.array(['1', 'o', '.', 'G']),
        np.array(['1', 'o', '.', 'P'])]

# no sets possible on the number characteristic
set6 = [np.array(['3', 's', ',', 'G']),
        np.array(['1', 'o', '#', 'R']),
        np.array(['1', 'o', '.', 'G']),
        np.array(['1', 'o', ',', 'P'])]
# This one has a valid set

test_morecards_set(set6)

# This should give True, True when things are working
test_morecards_set(set5)==0, test_morecards_set(set6)==1

set7 = [np.array(set5[0:]),
        np.array(set5[0]),
        np.array(set5[1])]
set7
(set5[1:])

"""**Question 3** Estimate the probability that three cards contain a set by recording the results of
running one of your functions above 10,000 times.  (This is theoretically 1/79)
"""

# solution
results = []
for i in range(10000):
    threecards = random.sample(list(universe), 3)  # returns a list of arrays..
    a= test_3cards_set(threecards)
    results.append(a)

1/79
random.sample(list(universe), 3)

np.sum(results), len(results)
np.mean(results)

"""**Question 4**  Estimate the proability that collections of 9, 10, 11, 12, 13, and 14 cards contain sets.

"""

def tester(cards):
  if (test_morecards_set(cards)==0):
    return 0
  else:
    return 1



test_morecards_set(threecards)

# solution
def probs(num):
  results1 = []
  for i in range(10000):
      threecards = random.sample(list(universe), num)  # returns a list of arrays..
      a= test_morecards_set(threecards)
      results1.append(a)

  return np.sum(results1)/len(results1)

print("probability of collections: \n3 = ", probs(3), "\n9 = ", probs(9),
      "\n10 = ", probs(10), "\n11 = ", probs(11), "\n12 = ", probs(12),
      "\n13 = ", probs(13), "\n14 = ", probs(14))

"""**Extra Credit**  Spreads of cards that do not contain a valid set are different from spreads of cards that contain one.  See if you can estimate the probability that adding an additional card to a nine-card spread that does not contain a set will create at least one new set."""

set9 = [np.array(['3', 's', ',', 'G']),
        np.array(['3', 's', '#', 'R']),
        np.array(['2', 's', '.', 'P']),
        np.array(['1', 'd', '.', 'P']),
        np.array(['3', 'o', '#', 'R']),
        np.array(['1', 's', '.', 'G']),
        np.array(['3', 'o', '#', 'P']),
        np.array(['2', 'd', '.', 'G']),
        np.array(['1', 's', '.', 'P'])]

test_morecards_set(set9)
#findsubsets(set9, 3)

results3 = []
for i in range(10000):
    tencards = set9 + random.sample(list(universe), 1)
    b= tester(tencards)
    a= tester(set9)
    c= b >= a
    results3.append(c)

np.sum(results3)/len(results3)
#threecards, tencards

"""## 3. Cryptocurrencies (20 points)

Imagine you're an investor in December 2017. Cryptocurrencies, online currencies backed by secure software, are becoming extremely valuable, and you want in on the action!

The two most valuable crpytocurrencies are Bitcoin (BTC) and Ethereum (ETH). Each one has a dollar price attatched to it at any given moment in time. For example, on December 1st, 2017, one BTC costs $\$$10859.56 and one ETH costs $\$$424.64.

**You want to predict the price of ETH at some point in time based on the price of BTC.**

There is a larger datset on [Kaggle](https://www.kaggle.com/jessevent/all-crypto-currencies/data) from which we obtained two tables called `btc` and `eth`. Each has 5 columns:
* `date`, the date
* `open`, the value of the currency at the beginning of the day
* `close`, the value of the currency at the end of the day
* `market`, the market cap or total dollar value invested in the currency
* `day`, the number of days since the start of our data
"""

btc_df=pd.read_csv('/btc.csv')
btc_df.head(10)

eth_df=pd.read_csv('/eth.csv')
eth_df.head(10)

"""#### Question 1

In some of the questions below, we will need to pair the daily 'btc' and 'eth' price data. A simple way is to use the corresponding columns in the two data frames (assuming that they correspond to the same dates). Verify that all the rows in the two data frames correspond to the same dates. You can see this for the first 10 observations by looking at the date columns above.


"""

btchead = np.array(btc_df.head(10))
ethhead = np.array(eth_df.head(10))
btceqeth = (btchead[:,0] == ethhead[:,0])
print (btceqeth)

"""
#### Question 2

In the cell below, make one or two plots to investigate the opening prices of BTC and ETH as a function of time. Then comment on whether you think the values are correlated."""

btc_df.plot('date', 'open')
plt.ylabel("BTC Opening Prices")
plt.xlabel("Date")
plt.scatter(btc_df["date"], btc_df["open"])
eth_df.plot('date', 'open', c="red")
plt.ylabel("ETH Opening Prices")
plt.xlabel("Date")
#plt.figure(figsize=(6,6))
plt.scatter(eth_df["date"], eth_df["open"], c="red")

"""I think they are correlated

#### Question 3

Now, calculate the correlation coefficient between the opening prices of BTC and ETH.
"""

def standard_units(x):
    "Convert any array of numbers to standard units."
    return (x - np.average(x))/np.std(x,ddof=1)
def correlation(x, y):
    x_in_standard_units = standard_units(x)
    y_in_standard_units = standard_units(y)
    if len(x)!=len(y):
        raise ValueError('arrays are of different lengths')
    return sum(x_in_standard_units * y_in_standard_units)/(len(x)-1)

btcopen = np.array(btc_df)[:,1]
ethopen = np.array(eth_df)[:,1]

correlation(btcopen, ethopen)

"""#### Question 4
Regardless of your conclusions above, write a function `eth_predictor` which takes an opening BTC price and predicts the price of ETH. Again, it will be helpful to use the function `regression_parameters` that you defined earlier in this homework.
"""

def eth_predictor(btc_price):
    parameters = line_mse(eth_df['open'], btc_df['open']).x
    slope = parameters[0]
    intercept = parameters[1]
    return (btc_price-intercept)/slope
eth_predictor(btc_df['open'])

"""#### Question 5

Now, using the `eth_predictor` you defined in the previous question, make a scatter plot with BTC prices along the x-axis and both real and predicted ETH prices along the y-axis. The color of the dots for the real ETH prices should be different from the color for the predicted ETH prices.

Hints:
* An example of such a scatter plot is generated <a href= "https://inferentialthinking.com/chapters/15/2/Regression_Line.html"> here. </a>
* Think about the table or data frame that must be produced and used to generate this scatter plot. What data should the columns represent? Based on the data that you need, how many columns should be present? Also, what should each row represent? Constructing the table or data frame will be the main part of this question; once you have this, generating the scatter plot should be straightforward as usual.
"""

plt.scatter(btc_df["open"], eth_df["open"], c="red", label='Real ETH')
plt.scatter(btc_df["open"], eth_predictor(btc_df['open']), c="blue", label='Predicted ETH')
plt.ylabel("ETH Opening Prices")
plt.xlabel("BTC Opening Prices")
plt.legend()

"""#### Question 6
For what values of BTC should you be more confident of your predictions? Considering the shape of the scatter plot of the true data, is the model we used reasonable? If so, what features make this model reasonable? If not, what features make it unreasonable?

I believe the model is reasonable for 0-10000 but past that point the features become more varied making it less reasonable.

#### Question 7

Now suppose you want to go the other way: to predict a BTC  price given an ETH price. Which parameters among the return values of `regression_parameters` are the same for the BTC-vs-ETH regression?
"""



"""I would assume that the slope is the same.

## 4. Simulations and Confidence Intervals (5 points)

In this exercise we will construct confidence intervals for the population correlation parameter and we will use simulations to assess the validity of our procedure.

Reminder:  the bootstrap technique
* takes multiple samples
* from the empirical distribution (the observed sample frequencies)
* of the same size as the original sample
* and uses these "noisy" artificial samples to model the distribution of sampling statistics.

The following cell contains a function that generates a dataset of sample size n from a population with correlation parameter equal to r.
"""

def cor_sim(r,n):
    x = np.random.normal(0, 1, n)
    z = np.random.normal(0, 1, n)
    y = r*x + np.sqrt(1 - r**2)*z  #  Weighted sum of x and z
    df = pd.DataFrame()
    df['X'] = x
    df['Y'] = y
    return df

# run this to get a scatter plot of a simulated dataset
sim1_df=cor_sim(0.3,200)

plt.figure(figsize=(5,5))
plt.scatter(sim1_df.X, sim1_df.Y, s=10,c='darkblue')

"""#### Question 8

Write a function that calculates (for two given variables reflected in two columns of a data frame) a 95% confidence interval for the population correlation using bootrap sampling.  Then calculate a 95% CI for a dataset simulated with the function above.
"""

#  a function that outputs a 95% CI for the correlation
# input is a data frame, the name of the two columns, and number of bootstraps
# output is an array of length three containing the correlation, and also
# the left and right ends of the 95% CI

def slope(x, y):
    if len(x)!=len(y):
        raise ValueError('arrays are of different lengths')
    return  correlation(x, y)* np.std(y,ddof=1)/np.std(x,ddof=1)
def bootstrap_slope(df, x, y, repetitions):
    n=df.shape[0]
    # Bootstrap the scatter, find the slope, collect
    slopes = np.array([])
    for i in np.arange(repetitions):
        bootstrap_sample = df.sample(n,replace=True)
        bootstrap_slope = slope(bootstrap_sample[x], bootstrap_sample[y])
        slopes = np.append(slopes, bootstrap_slope)

    # Find the endpoints of the 95% confidence interval for the true slope
    left = np.percentile(slopes,2.5)
    right = np.percentile(slopes,97.5)

    # Slope of the regression line from the original sample
    observed_slope = slope(df[x],df[y])

    # Display results
    print(np.array([observed_slope, left, right]))
    print('Slope of regression line:', round(observed_slope,3))
    print('Approximate 95%-confidence interval for the slope of the true line:')
    print(round(left,3), 'to', round(right,3))

# run this cell to get a confidence interval for one simulated dataset

# a simulated data frame with population correlation 0.3
sim1_df=cor_sim(0.3,200)

bootstrap_slope(sim1_df,'X','Y',1000)

"""#### Question 9

Let us check that the bootstrap confidence bounds are correct.

Repeat this procedure (generate a random dataset) 100 times with the same specifications (n=200, r=0.3) and construct a data frame that has 100 rows (one for each simulation) and three columns:

- ObsCor: which contains the observed correlations in each simulated datasets;
- Left: contains the left ends of the 95% confidence intervals;
- Right: contains the right ends of the 95% confidence intervals.
"""

def cors_sim(r,n):
    x = np.random.normal(0, 1, n)
    z = np.random.normal(0, 1, n)
    y = r*x + np.sqrt(1 - r**2)*z  #  Weighted sum of x and z
    obscor = correlation (x, y)
    left = np.percentile((x,y),2.5)
    right = np.percentile((x,y),97.5)
    df = pd.DataFrame()
    df['X'] = x
    df['Y'] = y
    df['obscor'] = obscor
    df['left'] = left
    df['right'] = right
    return np.array([obscor, left, right])

#program shut down

#np.array([cors_sim(.3, 200), cors_sim(.3, 200)])
#df2 = pd.DataFrame([cors_sim(.3, 200)])

#def rep(arr, num):
#  if num<101:
#    return rep((np.array([cors_sim(.3, 200)[0],cors_sim(.3, 200)[1],cors_sim(.3, 200)[2]]), np.array([arr])) ,num+1)
#  else:
#    return np.array([arr])
#rep(df2, 0)

#((np.array([cors_sim(.3, 200)[0],cors_sim(.3, 200)[1],cors_sim(.3, 200)[2]]),
#np.array([cors_sim(.3, 200)[0],cors_sim(.3, 200)[1],cors_sim(.3, 200)[2]])))

"""#### Question 10

Calculate the number of intervals that contain the population value of the correlation (r=0.3). Is this number consistent to your expectation?
"""

# calculate number of intervals
...

"""*Answer here:*  

"""



"""# Homework 2: Multiple Linear Regression (50 points)

**Reading**: Data8 textbook chapters [15](https://www.inferentialthinking.com/chapters/15/Prediction) and  [16](https://www.inferentialthinking.com/chapters/16/Inference_for_Regression).

Please complete this notebook by filling in the cells provided. Before you begin, execute the following cell to load the needed functions. Each time you start your server, you will need to execute this cell again to load them.

Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.


Homework 2 is due on Friday Wednesday January 25, 2022 at 11:59pm.
"""

# Commented out IPython magic to ensure Python compatibility.
# Don't change this cell; just run it.
import pandas as pd
import numpy as np
import matplotlib
# %matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

"""## 1. Birth Weight (50 points) ##

The data below contains the following variables for 1,174 mother-baby pairs: the babyâs birth weight in ounces, the number of gestational days, the motherâs age in completed years, the motherâs height in inches, pregnancy weight in pounds, and whether or not the mother smoked during pregnancy.


This dataset was used last quarter, but the only question investigated there was on the association of maternal smoking with birth weight. Here we will try to provide a better understanding of the factors that influence birth weight.

**The variable of interest is Birth Weight.**
"""

# this commands loads the data into a data frame
baby_df=pd.read_csv("/baby.csv")
baby_df.head(10)

"""#### Question 1 ( Visualization, 10 points)

Draw scatter plots of Birth Weight (on the y-axis) versus the four quantitative variables. Also construct a graph that shows the relation between Birth Weight and Maternal Smoker (Hint: boxplots, histograms, or violin plots).  
Why do Maternal Height and Maternal age have stripes but gestational days and pregnancy weight do not?
"""

baby_df["Maternal Smoker"].mean(), baby_df["Birth Weight"].mean()

len(baby_df)

"""####"""

plt.figure(figsize=(19,10))
plt.scatter(baby_df["Gestational Days"], baby_df["Birth Weight"], c="blue")
plt.scatter(baby_df["Maternal Age"], baby_df["Birth Weight"], c="red")
plt.scatter(baby_df["Maternal Height"], baby_df["Birth Weight"], c="purple")
plt.scatter(baby_df["Maternal Pregnancy Weight"], baby_df["Birth Weight"], c="green")
plt.ylabel("Birth Weight")
plt.xlabel("Quantitative Variables")
plt.title("Question One")
#plt.scatter(btc_df["date"], btc_df["open"])
plt.figure(figsize=(18, 24))

plt.subplot(3, 2, 1)
plt.scatter(baby_df["Gestational Days"],baby_df["Birth Weight"], c="blue")
plt.ylabel("Birth Weight")
plt.xlabel("Gestational Days")
plt.title('Gestational Days')

plt.subplot(3, 2, 2)
plt.scatter(baby_df["Maternal Age"],baby_df["Birth Weight"], c="red")
plt.ylabel("Birth Weight")
plt.xlabel("Maternal Age")
plt.title('Maternal Age')

plt.subplot(3, 2, 3)
plt.scatter(baby_df["Maternal Height"],baby_df["Birth Weight"], c="purple")
plt.ylabel("Birth Weight")
plt.xlabel("Maternal Height")
plt.title('Maternal Height')

plt.subplot(3, 2, 4)
plt.scatter(baby_df["Maternal Pregnancy Weight"],baby_df["Birth Weight"], c="green")
plt.ylabel("Birth Weight")
plt.xlabel("Maternal Pregnancy Weight")
plt.title('Maternal Pregnancy Weight')

#plt.hist(baby_df["Maternal Smoker"], baby_df["Birth Weight"], c="blue")

"""**For Questions 2-6, you should write your own code (it is fine to copy functions that we wrote for the class/lecture notebooks); do not use existing regression functions in Python libraries. (Optimization or linear algebra solvers are ok.)  All relevant code is in class notebooks.**

*Write your answer here, replacing this text.*
"""

#from class/lecture notebooks
def standard_units(x):
    "Convert any array of numbers to standard units."
    return (x - np.average(x))/np.std(x,ddof=1)

def correlation(x, y):
    x_in_standard_units = standard_units(x)
    y_in_standard_units = standard_units(y)
    if len(x)!=len(y):
        raise ValueError('arrays are of different lengths')
    return sum(x_in_standard_units * y_in_standard_units)/(len(x)-1)

def slope(x, y):
    if len(x)!=len(y):
        raise ValueError('arrays are of different lengths')
    return  correlation(x, y)* np.std(y,ddof=1)/np.std(x,ddof=1)

def intercept(x, y):
    b1 = slope(x, y)
    return np.average(y) - b1 * np.average(x)

def fitted_values(x, y):
    """Return an array of the regressions estimates at all the x values"""
    b1 = slope(x, y)
    b0 = intercept(x, y)
    return b1*x + b0

def residuals(x, y):
    return y - fitted_values(x, y)

#from class/lecture notebooks
# this is a function that outputs a 95% CI for the slope
# input is a data frame, the name of the two columns, and nr of bootstraps
def bootstrap_slope(df,x, y, repetitions):
    # the number of observations
    n=df.shape[0]
    # Bootstrap the scatter, find the slope, collect
    slopes = np.array([])
    for i in np.arange(repetitions):
        bootstrap_sample = df.sample(n,replace=True)
        bootstrap_slope = slope(bootstrap_sample[x], bootstrap_sample[y])
        slopes = np.append(slopes, bootstrap_slope)

    # Find the endpoints of the 95% confidence interval for the true slope
    left = np.percentile(slopes,2.5)
    right = np.percentile(slopes,97.5)

    # Slope of the regression line from the original sample
    observed_slope = slope(df[x],df[y])

    # Display results
    print('Approximate 95%-confidence interval for the slope of the true line:')
    print(left, 'to', right)


def prediction_at(df, x, y, value_of_x):
    b1 = slope(df[x],df[y])
    b0 = intercept(df[x],df[y])
    return b1 * value_of_x + b0


# function that outputs the CI for mean response
def bootstrap_mean_responseCI(df, x, y, new_x, repetitions):
    # the number of observations
    n=df.shape[0]
    # Bootstrap the scatter, predict, collect
    predictions = np.array([])
    for i in np.arange(repetitions):
        resample = df.sample(n,replace=True)
        predicted_y = prediction_at(resample, x, y, new_x)
        predictions= np.append(predictions, predicted_y)

    # Find the ends of the approximate 95% prediction interval
    left = np.percentile(predictions,2.5)
    right = np.percentile(predictions,97.5)

    return [left,right]

# prediction CI for a single observation
def bootstrap_predictionCI(df, x, y, new_x, repetitions):
    # Slope and intercept of the regression line from the original sample
    observed_slope = slope(df[x],df[y])
    observed_intercept = intercept(df[x],df[y])

    # the number of observations
    n=df.shape[0]

    # Bootstrap the scatter, predict, collect
    prediction_error = np.array([])
    for i in np.arange(repetitions):
        resample = df.sample(n,replace=True)
        boot_slope=slope(resample[x],resample[y])
        boot_intercept=intercept(resample[x],resample[y])
        predicted_y = prediction_at(resample, x, y, new_x)
        res=np.random.choice(residuals(resample[x],resample[y]),1)
        er=(observed_intercept-boot_intercept)+(observed_slope-boot_slope)*new_x+res
        prediction_error= np.append(prediction_error, er)

    # Find the ends of the approximate 95% prediction interval
    left = np.percentile(prediction_error, 2.5)
    right = np.percentile(prediction_error, 97.5)

    predicted_y = prediction_at(df, x, y, new_x)

    return [predicted_y+left,predicted_y+right]

def bootstrap_mean_response(df, x, y, new_x, repetitions):
    # the number of observations
    n=df.shape[0]
    # Bootstrap the scatter, predict, collect
    predictions = np.array([])
    for i in np.arange(repetitions):
        resample = df.sample(n,replace=True)
        predicted_y = prediction_at(resample, x, y, new_x)
        predictions= np.append(predictions, predicted_y)

    # Find the ends of the approximate 95% prediction interval
    left = np.percentile(predictions,2.5)
    right = np.percentile(predictions,97.5)

    # Display results
    plt.hist(predictions,bins=20)
    plt.xlabel('prediction at x='+str(new_x))
    plt.plot([left, right], [0, 0], color='yellow', lw=8);
    print('Approximate 95%-confidence interval for height of true line:')
    print(left, right, '(width =', right - left, ')')

from scipy.optimize import minimize

"""#### Question 2 (Simple Linear Regression, Gestational Age, 5 points)

Fit a simple linear regression model for `Gestational Days` (as the predictor) and `Birth Weight` (as the response). Calculate the intercept, the slope and a 95% Confidence Interval for the slope. Briefly discuss your results by: (i) interpreting the slope; (ii) discussing the signficance of the predictor (is there evidence that the slope is different than 0?); (iii) commenting on whether the results make sense (does it make sense for birth weight to be associated with gestational age?).


"""

# code here and in additional cells if needed
def fit(x,y):
    m=slope(x,y)
    b=intercept(x,y)
    return(m,b)
fit(baby_df["Gestational Days"],baby_df["Birth Weight"])
print('slope, intercept:', fit(baby_df["Gestational Days"],baby_df["Birth Weight"]))
print(bootstrap_slope(baby_df,'Gestational Days','Birth Weight',1000))
plt.scatter(baby_df["Gestational Days"], baby_df["Birth Weight"])
xlims=np.array([120,360])
plt.plot(xlims,0.4665568769492154*xlims,-10.754138914450309,color="red",lw=2);
plt.xlabel("Birth Weight")
plt.ylabel("Gestational Days")
plt.title("Simple Linear Regression of Gestational Days")

#def baby_quadratic_mse(param):
#    x = baby_df["Gestational Days"]
#    y = baby_df["Birth Weight"]
#    estimate = param[0]*(x**2) + param[1]*x + param[2]
#    return np.mean((y - estimate) ** 2)
#best_fit=minimize(baby_quadratic_mse,x0=[0,0,0],method="BFGS").x
#baby_fit=best_fit[0]*(baby_df["Gestational Days"]**2)+best_fit[1]*baby_df["Gestational Days"]+best_fit[2]
#plt.scatter(baby_df["Gestational Days"], baby_df["Birth Weight"])
#xlims=np.array([])
#x=np.arange(140,380,10)
#plt.plot(x,best_fit[0]*(x**2)+best_fit[1]*x+best_fit[2],color="red");

"""#### Question 3 (Simple Linear Regression, Maternal Age, 5 points)

Repeat Question 2 for Maternal Age.

"""

fit(baby_df["Maternal Age"],baby_df["Birth Weight"])
print('slope, intercept:', fit(baby_df["Maternal Age"],baby_df["Birth Weight"]))
print(bootstrap_slope(baby_df,'Maternal Age','Birth Weight',1000))
#plt.scatter(baby_df["Maternal Age"],baby_df["Birth Weight"])
#xlims=np.array([10,50])
#plt.plot(xlims,0.08500766941582513*xlims,117.14790872185156,color="red",lw=2);
#plt.xlabel("Birth Weight")
#plt.ylabel("Maternal Age")
#plt.title("Simple Linear Regression of Maternal Age")

def baby_quadratic_mse(param):
    x = baby_df["Maternal Age"]
    y = baby_df["Birth Weight"]
    estimate = param[0]*(x**2) + param[1]*x + param[2]
    return np.mean((y - estimate) ** 2)
best_fit=minimize(baby_quadratic_mse,x0=[0,0,0],method="BFGS").x
baby_fit=best_fit[0]*(baby_df["Maternal Age"]**2)+best_fit[1]*baby_df["Maternal Age"]+best_fit[2]
plt.scatter(baby_df["Maternal Age"], baby_df["Birth Weight"])
xlims=np.array([])
x=np.arange(13,48,3)
plt.ylabel("Birth Weight")
plt.xlabel("Maternal Age")
plt.title("Simple Linear Regression of Maternal Age")
plt.plot(x,best_fit[0]*(x**2)+best_fit[1]*x+best_fit[2],color="red");

print([slope(baby_df["Maternal Age"], baby_df["Birth Weight"]),intercept(baby_df["Maternal Age"], baby_df["Birth Weight"])])
# residuals plot
plt.scatter(baby_df["Maternal Age"], residuals(baby_df["Maternal Age"],baby_df["Birth Weight"]))
plt.plot([10,50],[0,0],color="black",lw=1);
xlims=np.array([10,50])
plt.plot(xlims,0.08500766941582513*xlims,117.14790872185156,color="red",lw=2);
plt.ylabel("Birth Weight")
plt.xlabel("Maternal Age")
plt.title("Simple Linear Regression of Maternal Age")

"""*Write your answer here, replacing this text.*

#### Question 4 (Simple Linear Regression, Maternal Height, 5 points)

Repeat Question 2 for Maternal Height and plot the residuals.
"""

# code here and in additional cells if needed
# residuals plot
fit(baby_df["Maternal Height"],baby_df["Birth Weight"])
print('slope, intercept:', fit(baby_df["Maternal Height"],baby_df["Birth Weight"]))
print(bootstrap_slope(baby_df,'Maternal Height','Birth Weight',1000))
plt.scatter(baby_df["Maternal Height"], residuals(baby_df["Maternal Height"],baby_df["Birth Weight"])/1000)
plt.plot([50,75],[0,0],color="black",lw=1);
plt.ylabel("Birth Weight")
plt.xlabel("Maternal Height")
plt.title("Simple Linear Regression of Maternal Height")
# residuals plot

"""*Write your answer here, replacing this text.*

#### Question 5 (Simple Linear Regression, Maternal Smoking, 5 points)

Fit a simple linear regression model with `Maternal Smoker` as the predictor (x variable). You need to create a dummy variable, a numerical variable with values 0 and 1 for that.

What is the meaning of slope and intercept here?
"""

fit(baby_df["Maternal Smoker"],baby_df["Birth Weight"])
print('slope, intercept:', fit(baby_df["Maternal Smoker"],baby_df["Birth Weight"]))
print(bootstrap_slope(baby_df,'Maternal Smoker','Birth Weight',1000))
plt.scatter(baby_df["Maternal Smoker"], residuals(baby_df["Maternal Smoker"],baby_df["Birth Weight"])/1000)
plt.plot([50,75],[0,0],color="black",lw=1);
plt.ylabel("Birth Weight")
plt.xlabel("Maternal Smoker")
plt.title("Simple Linear Regression of Maternal Smoker")
# residuals plot

plt.scatter(baby_df["Maternal Smoker"], baby_df["Birth Weight"],c="darkblue",s=25);


plt.grid()
plt.scatter(np.log(baby_df["Maternal Smoker"]),
              residuals(np.log(baby_df["Maternal Smoker"]),np.log(baby_df["Birth Weight"])),c="black",s=25)
plt.plot([.5,1],[0,0],color="black",lw=1);

"""#### Question 6 (Multiple Linear Regression, 10 points)

Fit a multiple linear regression with four predictors: Gestational Days, Maternal Smoking, Maternal Height and Maternal Pregagncy Weight.

To do this, you should probably write a loss function that returns a single value and run scipy.optimize.minimize().  

Here you are required only to find the least square parameters (the value of the parameters that minimize the least squares function). Discuss whether these parameters have changed dramatically from when you fitted simple linear regressions (for the three variables for which you did that above).
"""

# code here and in additional cells if needed
# Y = b0 + b1 x1 + b2 x2 + ... +  bp xp + Îµ
n=baby_df.shape[0]

predicted1 = np.array([])

for i in range(n):
    # drop the i-th row
    tmp_df=baby_df.drop(i)

    # this is residual sum of squares function
    def rss(b):
        estimate = b[0]+b[1]*tmp_df["Gestational Days"]+b[2]*tmp_df["Maternal Age"]+b[3]*tmp_df["Maternal Height"]+b[4]*tmp_df["Maternal Pregnancy Weight"]
        return (np.mean((tmp_df["Birth Weight"] - estimate) ** 2))

    # the estimated values of the parameters
    tmp_fit=minimize(rss,x0=[0,0,0,0,0],method="BFGS").x

    #the prediction
    tmp_pred=tmp_fit[0]+tmp_fit[1]*baby_df["Gestational Days"][i] \
            +tmp_fit[2]*baby_df["Maternal Age"][i]+tmp_fit[3]*baby_df["Maternal Height"][i]+tmp_fit[4]*baby_df["Maternal Pregnancy Weight"][i]

    # appending the result
    predicted1 = np.append(predicted1, tmp_pred)

predicted1[0:10]

print("values of the parameters", tmp_fit)

"""values of the parameters [-8.59599419e+01  4.56118556e-01  1.21719794e-01  1.02772043e+00
  6.99811859e-02]

#### Question 7 (Prediction and cross validation, 10 points)

Compare two models to predict birth weight.

Assess the performance of two models using "leave one out" cross validation, where you fit the predictor on n-1 data and evaluate its accuracy predicting the "held out" data point.

You should do this for two models: (i) the model in Question 6; (ii) a model of your choice. For the model of your choice, pick something that makes sense to you (you will not be evaluated on whether it predicts better than the model in Question 6).

**For computational convenience, you may do the cross-validation only for a subset of the data, let's say the first 100 subjects.**

**For Question 7 you are allowed to use available regression functions if you find them useful (for computational reasons).**

This model presents a graph of Birth Weight vs Maternal Age Predications as well as a of Birth Weight vs the previous model. It also gives the slope, intercept, mean relative error, and mean absolute deviation for the predictions.
"""

print(np.mean(abs(residuals(baby_df["Maternal Age"],baby_df["Birth Weight"]))))
print(np.mean(abs(residuals(baby_df["Maternal Age"],baby_df["Birth Weight"]))/baby_df["Birth Weight"]))
n=baby_df.shape[0]

predicted3 = np.array([])

for i in range(n):
    # drop the i-th row
    tmp_df=baby_df.drop(i)

    #the predicted price
    tmp_pred=intercept(tmp_df["Maternal Age"],tmp_df["Birth Weight"]) \
            +slope(tmp_df["Maternal Age"],tmp_df["Birth Weight"])*baby_df["Maternal Age"][i]

    # appending the result
    predicted3 = np.append(predicted3, tmp_pred)

predicted3[0:100]
print("mean absolute deviation for prediction:", np.mean(abs(baby_df["Birth Weight"]-predicted3)),
      "\nmean relative error:", np.mean(abs(baby_df["Birth Weight"]-predicted3)/baby_df["Birth Weight"]),
      "\nslope:", slope(predicted3,baby_df["Birth Weight"]),
       "\nintercept:", intercept(predicted3,baby_df["Birth Weight"]))

plt.scatter(predicted3,baby_df["Birth Weight"])
plt.plot([117,122],[110,130],color="red",lw=2)
plt.title('Birth Weight (x) versus Maternal Age prediction (y)');

#function defined in problem above
print("mean absolute deviation for prediction:", np.mean(abs(baby_df["Birth Weight"]-predicted1)),
      "\nmean relative error:", np.mean(abs(baby_df["Birth Weight"]-predicted1)/baby_df["Birth Weight"]),
      "\nslope:", slope(predicted1,baby_df["Birth Weight"]),
       "\nintercept:", intercept(predicted1,baby_df["Birth Weight"]))
plt.scatter(predicted1,baby_df["Birth Weight"])
plt.plot([50,180],[50,180],color="red",lw=2)
plt.title('Birth Weight (x) versus Model 1 prediction (y)');

"""mean absolute deviation for prediction and mean relative error using the Maternal Age was closer to that calculated with the residuals"""



"""# Homework 3: Data Cleaning  (25 points, short homework)
## Data 119, Winter 2023  Sec 2 Trimble/Nussbaum

**Pandas Cheat Sheet**: There are several Pandas documentation files you can find with a simple search. This is short and informative: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf

Please complete this notebook by filling in the cells provided. Before you begin, execute the following cell to load the needed functions. Each time you start your server, you will need to execute this cell again to load them.  

Homework 3 is due on Friday January 2/27 at 11:59pm.
"""

# Commented out IPython magic to ensure Python compatibility.
# Don't change this cell; just run it.

import numpy as np
import matplotlib
# %matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

import pandas as pd

"""## Chicago Divvy Data ##

For this exercise, we downloaded the Chicago Divvy data for the last quarter before the pandemic (Q4 of 2019) from:

https://divvy-tripdata.s3.amazonaws.com/index.html

The map of the system is here:
https://member.divvybikes.com/map/

The list of stations can be found here:
https://data.cityofchicago.org/Transportation/Divvy-Bicycle-Stations/bbyy-e7gq/data

The downloaded dataset has more than 700K rides, and the provided data contains only a subset of the 2400 rides that started or ended around Hyde Park.
"""

# Don't change this cell; just run it.
# The code assumes that the Bike2.csv file is in the "data" directory,
# which is located in the parent directory of the notebook
bike=pd.read_csv("/Bike2-1.csv")
bike.head(10)

"""### Question 1

The file as provided has some duplicate entries (i.e., duplicate rows). A useful pandas tool for dealing with duplicates can be found here:

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html

Write a function to compute the number of duplicated rows in the dataset.  How many are there?
"""

## Answer here and in additional cells if needed
cam=bike.drop_duplicates()
cam.count()
cam.shape[0], bike.shape[0]
print("Number of duplicated rows in the dataset when keeping only one value:",
      bike.shape[0]-cam.shape[0],
      "\nNumber of duplicated rows in the dataset when keeping no values:",
      (2*(bike.shape[0]-cam.shape[0])))

"""### Question 2

Create a data frame called `bikeND` that removes the duplicates and has only unique entries.
Also, calculate the number of unique entries in the `trip_id` column and verify that it is the same as the number of rows.
"""

## Answer here and in additional cells if needed
bikeND=cam
len(bikeND)
bikeND['trip_id'].nunique()
print("# of bikeND rows equal to # of unique trip_id entries?: ",
      len(bikeND)==bikeND["trip_id"].nunique())

"""### Question 3

For this and the remaining questions, we will work with the data frame that has unique rows: `bikeND`.

We will now investigate the missing values in the data. Answer these:
1. How many values are missing for each column?
2. How many rows have one or more missing values?
3. How do missing values differ by user type?

"""

## Answer here and in additional cells if needed
def missing_value(hi):
  return(len(bikeND)-len((bikeND.iloc[:, [hi]]).dropna()))
Subscribers=bikeND.loc[bikeND['usertype'] == "Subscriber", :]
len(Subscribers)-len(Subscribers.dropna())
Customers=bikeND.loc[bikeND['usertype'] == "Customer", :]
len(Customers)-len(Customers.dropna())

print("Number of values missing for each bikeND column:",
      "\n trip_id = ", missing_value(0),
      ", start_time = ", missing_value(1),
      ", end_time = ", missing_value(2),
      "\n bikeid = ", missing_value(3),
      ", tripduration = ", missing_value(4),
      ", from_station_id = ", missing_value(5),
      "\n from_station_name = ", missing_value(6),
      ", to_station_id = ", missing_value(7),
      ", to_station_name = ", missing_value(8),
      "\n usertype = ", missing_value(9),
      ", gender = ", missing_value(10),
      ", birthyear = ", missing_value(11),
      "\n\nNumber of rows that have one or more missing values:",
      len(bikeND)-len(bikeND.dropna()),
      "\n\nNumber of missing values that differ by user type: Out of the",
      len(bikeND)-len(bikeND.dropna()),
      "missing values,\nthe usertype of customer has",
      len(Customers)-len(Customers.dropna()),
      "of those values and the usertype of subscriber has",
      len(Subscribers)-len(Subscribers.dropna()))

"""### Question 4

Recover some of the missing data.  Some of the rows have corrupt values for `from_station_name` and `to_station_name`.  Write code to fill in as many of the missing data elements as possible.  The `station_id` fields have not been corrupted.  

How many trips started from University Ave & 57th St ?

Hint:  You probably want to build a data structure to help you (a dictionary would be a good choice).  You probably also want a function to access the contents of the data structure and apply it to your dataframe.
"""

station_info = bikeND.drop(columns=['trip_id', 'start_time', 'end_time',
                                    'bikeid', 'tripduration', 'usertype',
                                    'gender', 'birthyear'])
from_station_info = station_info.drop(columns=['to_station_id',
                                               'to_station_name'])
to_station_info = station_info.drop(columns=['from_station_id',
                                             'from_station_name'])
from1= from_station_info.rename(columns={'from_station_id':'station_id',
                                         'from_station_name':'station_name'})
from2= to_station_info.rename(columns={'to_station_id':'station_id',
                                       'to_station_name':'station_name'})
station_ids=pd.merge(from1, from2, how='outer').drop_duplicates()
station_ids['station_id'].nunique() #96
station_ids_names= station_ids.dropna()
station_ids_names
station_dic=dict(zip(station_ids_names.station_name,
                     station_ids_names.station_id))
station_dic2=dict(zip(station_ids_names.station_id,
                      station_ids_names.station_name))
station_dic

def new_station_info (okay, notokay):
  old_station_info=notokay.replace({okay:station_dic2})
  return old_station_info.replace({okay:station_dic})

def new_station_info2 (okay, notokay):
  old_station_info=notokay.replace({okay: station_dic})
  return old_station_info.replace({okay:station_dic2})
#test=new_station_info2("from_station_id", station_id)
#test1=(new_station_info2("station_id", station_ids)).rename(columns=
#{'station_id':'station_name', 'station_name':'station_names'})
#final=pd.concat([test1.drop(columns=['station_names']),
#station_ids.drop(columns=['station_name'])], axis=1)
test1=(new_station_info2("from_station_id",
                         (station_info.drop
                          (columns=
                           ['to_station_id', "to_station_name"])))).rename(
                               columns={'from_station_id':'from_station_name',
                                     'from_station_name':'station_names'})
test2=(new_station_info2("to_station_id",
                         (station_info.drop
                          (columns=['from_station_id',
                                    "from_station_name"])))).rename(
                                        columns={'to_station_id':
                                              'to_station_name',
                                              'to_station_name':
                                              'station_names2'})
final=pd.concat([test2.drop(columns='station_names2'),
                 test1.drop(columns=['station_names']),
                 bikeND.drop(columns=['from_station_name',
                                      'to_station_name'])], axis=1)
print("Length of bikeND - length of final_bike:",len(bikeND)-len(final),"\n\n")

def missing_value2(hi):
  return(len(final)-len((final.iloc[:, [hi]]).dropna()))

print("Number of values missing for final bikeND column:",
      "\n trip_id = ", missing_value2(2),
      ", start_time = ", missing_value2(3),
      ", end_time = ", missing_value2(4),
      "\n bikeid = ", missing_value2(5),
      ", tripduration = ", missing_value2(6),
      ", from_station_id = ", missing_value2(7),
      "\n from_station_name = ", missing_value2(1),
      ", to_station_id = ", missing_value2(8),
      ", to_station_name = ", missing_value2(0),
      "\n usertype = ", missing_value2(9),
      ", gender = ", missing_value2(10),
      ", birthyear = ", missing_value2(11),
      "\n\nNumber of trips that started from University & 57th St:",
      final['from_station_id'].value_counts()[423])



"""# Homework 4: SQL   (Section 2 Trimble/Nussbaum)  (9 questions, 50 points)

**Pandas Cheat Sheet**: There are several Pandas documentation files you can find with a simple search. This is one that is short and informative: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf

**SQL Cheat Sheet**: There are lots of tutorials and such for SQL queries.  Some of them might have syntax that is slightly different than what is supported by SQLite.   In addition to the examples in the [examples in the DS100 textbook Ch 7](https://www.textbook.ds100.org/ch/07/sql_intro.html),, this site from CodeAcademy has a good summary of basic SQL statements:
https://www.codecademy.com/learn/learn-sql/modules/learn-sql-manipulation/cheatsheet

Please complete this notebook by filling in the cells provided. Before you begin, execute the following cell to load the needed functions. Each time you start your server, you will need to execute this cell again to load them.  

Homework 4 is due on Friday, February 3  at 11:59am.
"""

# Commented out IPython magic to ensure Python compatibility.
# Don't change this cell; just run it.

import numpy as np

import matplotlib
# %matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

import pandas as pd
import sqlalchemy
pd.__version__

"""The Internet Movie Database makes a handful of meager public use datasets that are built nightly.
https://www.imdb.com/interfaces/

We have extracted subsets of the basic movie info and rating information for movies from 2010 and placed these in a sqlite3 database-formatted file.

Here we ask you to use SQL to get out lists of movies and aggregate statistics of movies from this database.
"""

# Load database (with two tables) from file  imdb-2010.db

#Let's connect to our database system and clean up previous tables if they exist

# pd.read_sql takes in a parameter for a SQLite engine.
# This will try to load the file imdb-2010.db, which contains two tables.

sqlite_uri = "sqlite:///imdb-2010.db"


sqlite_engine = sqlalchemy.create_engine(sqlite_uri)

#start with an empty database - DROP the tables if they
#already exist in the database (from a previous run)
sql_expr = """
DROP TABLE IF EXISTS title_ratings;
"""
# result = sqlite_engine.execute(sql_expr)


sql_expr = """
DROP TABLE IF EXISTS title_basics;
"""
# result = sqlite_engine.execute(sql_expr)



"""#### Table Creation

**bold text**# This cell would populate the two tables from the datafiles title.basics.movies.2010.csv and ratings.2010.csv.
# You shouldn't need to use this but it may be useful for reference; has not been tested to preserve data types.

# Create a new SQLite engine:
sqlite_uri = "sqlite:///"

sqlite_engine = sqlalchemy.create_engine(sqlite_uri)

#start with an empty database - DROP the tables if they
#already exist in the database (from a previous run)
sql_expr = '''
DROP TABLE IF EXISTS title_ratings;
'''
result = sqlite_engine.execute(sql_expr)


sql_expr = '''
DROP TABLE IF EXISTS title_basics;
'''
result = sqlite_engine.execute(sql_expr)


#read csv file into a dataframe
basics_df = pd.read_csv('../data/title.basics.movies.2010.csv', sep="\t")

# Populate players table
temp = basics_df.to_sql('title_basics', con=sqlite_engine)

#read csv file into a dataframe
ratings_df = pd.read_csv('../data/ratings.2010.csv', sep="\t")

# Populate salaries table
temp = ratings_df.to_sql('title_ratings', con=sqlite_engine)

#### Check the tables
Before we start, let's look at what's in our tables
"""

!pip install --upgrade pandas
!pip install --upgrade 'sqlalchemy<2.0'

# This is usually SHOW TABLES; or .tables; but for some reason those don't work.

#This command lists the tables defined in the sqlite database:
sql_expr = '''
SELECT name
FROM sqlite_master
WHERE type = "table" AND name NOT LIKE 'sqlite_%';
'''
pd.read_sql(sql_expr, sqlite_engine)

# Select everything from the title_basics table and display it as a pandas dataframe:
sql_expr = """
SELECT *
FROM title_basics;
"""
pd.read_sql(sql_expr, sqlite_engine)

# Select everything from the title_basics table and put it in a pandas dataframe:
sql_expr = """
SELECT *
FROM title_ratings
"""
pd.read_sql(sql_expr, sqlite_engine)

"""### Question 1 (Basic Query, 5 points)

Write a SQL statement that returns the title, runtime, and genres for all
movies longer than three hours (runtimeMinutes is greater than 180), sorted in descending order by runtime.
"""

# Put your SQL statement in the "sql_expr" variable
sql_expr = """
SELECT DISTINCT primaryTitle,originalTitle, runtimeMinutes, genres
FROM title_basics
WHERE runtimeMinutes > 180
ORDER BY 3
"""

pd.read_sql(sql_expr, sqlite_engine)

"""#### Question 2 (Unique values, 5 points)

Write a SQL statement that returns the number of different values in the genres field.
How many different genre descriptions are there?

"""

# Put your SQL statement in the "sql_expr" variable

sql_expr = """
SELECT genres, COUNT(genres)
FROM title_basics
GROUP BY genres;
"""
print ('Number of genres:', len(pd.read_sql(sql_expr, sqlite_engine)))
pd.read_sql(sql_expr, sqlite_engine)

"""#### Question 3 (Another single-table query, 5 points)

Write a SQL statement that returns the name and runtime of every film in the dataset with "Drama" for genres.

"""

# Put your SQL statement in the "sql_expr" variable
sql_expr = """
SELECT DISTINCT primaryTitle,originalTitle, runtimeMinutes, genres
FROM title_basics
WHERE genres LIKE '%drama%';
"""

pd.read_sql(sql_expr, sqlite_engine)

"""#### Question 4 (Summary Statistics (aggregate functions), 5 points)
Write an SQL statement that reports the number of films in the database and the average length of all the films in the database.
 (Note: the SQL "AVG" aggregate function is helpful here)

"""

# Put your SQL statement in the "sql_expr" variable

sql_expr = """
SELECT COUNT(*) AS 'Number of films in database',
AVG(runtimeMinutes)
FROM title_basics

"""
pd.read_sql(sql_expr, sqlite_engine)

"""#### Question 5 (Grouping, 5 points)
Write a SQL statement that calculates the average length of a film by genre.

"""

# Put your SQL statement in the "sql_expr" variable

sql_expr = """
SELECT genres, Avg(runtimeMinutes), COUNT(*)
FROM title_basics
GROUP BY genres;

"""
pd.read_sql(sql_expr, sqlite_engine)

"""#### Question 6 (Grouping and Predicates, 5 points)
There are too many little categories here to make sense of.

Same as question 5, but limit the list to genre descriptions with more than 100 films.   (There are 19)

"""

# Put your SQL statement in the "sql_expr" variable

sql_expr = """
SELECT genres, Avg(runtimeMinutes), COUNT(*)
FROM title_basics
GROUP BY genres
HAVING COUNT(*)>100;

"""

pd.read_sql(sql_expr, sqlite_engine)

"""#### Question 7 (Joins (Inner), 5 points)
Write a SQL statement that returns a row for **each title who has a rating**, that has the title, runtime, number of ratings and rating of the movie  **descending order of rating**.

"""

# Put your SQL statement in the "sql_expr" variable

sql_expr = """
SELECT primaryTitle, originalTitle, runtimeMinutes, averageRating, numVotes
FROM title_ratings
JOIN title_basics
  ON title_ratings.tconst = title_basics.tconst
WHERE primaryTitle IS NOT NULL AND averageRating > 0
ORDER BY averageRating;

"""

pd.read_sql(sql_expr, sqlite_engine)

"""#### Question 8 (Joins (Outer), 5 points)
Write a SQL statement that returns a row **for each movie** **whether or not it has an imdb rating**, that has the title, runtime, rating, and number of ratings for the movie.   For movies without a rating, the rating and number of ratings fields can be left as a Null or "NaN". Order your answer in **descending order of rating**.
"""

# Put your SQL statement in the "sql_expr" variable

sql_expr = """
SELECT primaryTitle, originalTitle, runtimeMinutes, averageRating, numVotes
FROM title_basics
LEFT JOIN title_ratings
  on title_basics.tconst = title_ratings.tconst
ORDER BY averageRating;
"""


pd.read_sql(sql_expr, sqlite_engine)



"""#### Question 9 (Putting it all together, 10 points)


Write a SQL statement that returns a table with **a row for each genre with more than 50 movies**.  Each row should have the genre description, the number of movies, the average runtime, and the average rating, and the maximum number of ratings for any movie within the genre.

"""

# Put your SQL statement in the "sql_expr" variable
sql_expr = """
SELECT genres, COUNT(genres) 'Number of movies', AVG(runtimeMinutes) 'Average runtime',
AVG(averageRating) 'Average rating in genre', MAX(averageRating) 'Max number of rating'
FROM title_basics
LEFT JOIN title_ratings
  on title_basics.tconst = title_ratings.tconst
GROUP BY genres
HAVING COUNT(genres) > 50;
"""
pd.read_sql(sql_expr, sqlite_engine)



"""# Homework 5: Grad


"""

# Commented out IPython magic to ensure Python compatibility.
# Don't change this cell; just run it.
import pandas as pd
import numpy as np
import matplotlib
# %matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

## HW 5  - data cleaning, knn regression
#Data 119  Sec 2 (Trimble/Nussbaum)

#Please complete this notebook by filling in the cells provided.
#Before you begin, execute the following cell to load the needed functions.
#Each time you start your server, you will need to execute this cell
#again to load them.

#Homework 5 is due on Friday, February 10 at 11:59am.

"""#### 1. Intuition on classification (16 points)

#### Question 1.1 (4 points)
You have a large dataset `breast-cancer` which has 11 columns. The first ten are attributes of the person that might be predictive of whether or not someone has breast-cancer, and the 11th column indicates whether they have it or not. 99% of the table contains examples of people who do not have breast cancer.

Imagine you are trying to use a k-NN classifier to use the first ten columns to predict whether or not someone has breast cancer. You split your training and test set up as necessary, you develop a 7 Nearest Neighbors classifier, and you notice your classifier predicts every point in the test set to be a person who does not have breast cancer. Is there a problem with your code? Explain this phenomenon.

There's not necessarily an issue with your code instead the phenomenon could be that the equations used to determine the identifiers for breast cancer could not have a uniquely strong correlation specifically with breast cancer. By that I mean if for example, a signifier of breast cancer is more acidic cellular pH yet diabieties and kidney failure also cause a more acidic cellular pH and the attributes being used are similar, then it could be that these overlapping points won't necessarily suggest breast cancer.

#### Question 1.2 (4 points)

You have a training set of 35 examples of characteristics of fruits along with what fruit is actually being described. 25 of the examples of Apples, and 10 of the examples are Oranges.

You decide to make a k-NN classifier. Give the smallest possible choice for k such that the classifier will predict Apple for every point, regardless of how the data is spread out. Explain how you picked your k.

Imagine that ties are broken at random for even values of k, so there is no guarantee of what will be picked if there is a tie.

You would need k to be 21 because there will be 10 oranges and you need more apples than oranges to cancel out so 10 orange data points and 11 apple data points. 10+11=21 therefore 21 is the smallest vallue

## 2. Institute Graduation Rate Prediction Dataset (42 points) ##
There is a dataset on kaggle called "Institute Graduation Rate Prediction Dataset" prepared by Mala H. Mehta.  It reports graduation rates at community colleges. The dataset consists of total 143 features and 11319 records of 8 student batches between 2004 and 2011.  Each row seems to represent a cohort of students at an institution in a particular year.

https://www.kaggle.com/datasets/mehtamala/institute-graduation-rate-prediction-dataset
doi: 10.34740/KAGGLE/DSV/2914166

This dataset has some fields that are numbers (like tuition or enrollment), many that are indicator variables, and some fields are missing.  

In this homework we will try to predict the graduation rate by k-nearest-neighbors regression, will try to predict whether a row is in the upper half or the lower half of graudation rates, and we will report on the accuracy of our predictions.

GBA4RTT is the primary outcome variable; 4-year graduation rate.
"""

# this commands loads the data into a data frame
igr_df = pd.read_csv("/IGR_Prediction_DS.csv")
igr_df.head(10)

igr_df.shape  # 11318 rows, 143 columns including two response variables

"""#### Question 2.1 ( Binarization, 3 points)

Create a new column in the data frame containing a binary variable called **tophalf**: it should be equal to 1 if **GBA4RTT** contains a value above its median and equal to 0 if **GBA4RTT** contains a value below its median.
"""

# code here and in additional cells if needed
igr_df["tophalf"]=np.nan
igr_df[['tophalf', 'GBA4RTT']]
med = igr_df['GBA4RTT'].median()
#above = igr_df[igr_df.GBA4RTT > med]
#equall = igr_df[igr_df.GBA4RTT == med]
#below = igr_df[igr_df.GBA4RTT < med]
igr_df.loc[igr_df['GBA4RTT'] >= med, ['tophalf']] = 1
igr_df.loc[igr_df['GBA4RTT'] < med, ['tophalf']] = 0
print ('Values that are exactly the median will be counted as tophalf with value of 1')
igr_df

"""#### Question 2.2 (Data cleaning, 4 points)
Out of 141 fields, many are missing most of the values.  Make a copy of your dataframe that has had columns that are missing more than 20 values removed.

"""

small_igr_df = igr_df
clean_igr_df_1 = small_igr_df.dropna(axis=1, thresh=(11318-20))
clean_igr_df_1

"""#### Question 2.3 (Data cleaning, 2 points)
Of the remaining 54 fields, two columns have zero standard deviation and both won't help us but also will cause our tools to fail.  Find these columns and remove them.
"""

clean_igr_df_1.std()
#FTPT and FSTAT
clean_igr_df_2=clean_igr_df_1.drop(columns=['FTPT', 'FSTAT'])
clean_igr_df_2

"""#### Question 2.4  Graphs

Explore the data graphically (and numerically if you think it is useful).
Many variables in this dataset are indicator variables.  Find a relationship
that makes for an interesting graph.  Make a well-annotated graph and
describe your findings.

(Faculty gender vs. student gender ratio?  Tuition vs. faculty salaries?  
 School size vs. graduation rate?)
"""

group_by_age = pd.cut(clean_igr_df_2["AVESALT"], np.arange(0, 90, 10))
age_grouping = clean_igr_df_2.groupby(group_by_age).mean()
age_grouping['GBA4RTT'].plot.bar();
plt.scatter(clean_igr_df_2[clean_igr_df_2.tophalf==0]['GBA4RTT'],clean_igr_df_2[clean_igr_df_2.tophalf==0]['AVESALT'],color="blue")
plt.scatter(clean_igr_df_2[clean_igr_df_2.tophalf==1]['GBA4RTT'],clean_igr_df_2[clean_igr_df_2.tophalf==1]['AVESALT'],color="red");

"""#### Question 2.5  Correlations (3 points)
Evaluate the correlations between the outcome variable  and the remaining
predictor variables.  Find the eight columns with the strongest correlations
and look the field names up in the data dictionary.  
"""

(clean_igr_df_2.corr().mean().sort_values(ascending=False)).iloc[0:8]

"""#### Question 2.6 (Data standardizing, 3 points)

Subract the column-wise means and divide by column-wise standard deviations
to make all the remaining columns have mean 0 and standard deviation 1.  

This will make some downstream algorithms happier when comparing dollars
to indicator variables.
"""

def standard_units(x):
    "Convert any array of numbers to standard units."
    return (x - np.mean(x))/np.std(x,ddof=1)

# code here and in additional cells if needed
clean_igr_df_3 = clean_igr_df_2[['ANYAIDN', 'IGRNT_N', 'LOAN_N', 'EAPTYP', 'EMPCNTT', 'OUTLAYT', 'FGRNT_N', 'SGRNT_N']]
clean_igr_df_2.apply(standard_units).describe()
clean_igr_df_3.apply(standard_units).describe()
#(((clean_igr_df_3['ANYAIDN'] - np.mean(clean_igr_df_3.ANYAIDN)))/np.std(clean_igr_df_3.ANYAIDN)).mean()

"""#### Question 2.7 (Split the data, 3 points)

Split the data into a training set and a test set. **Randomly** select 20% of the rows of your data frame and create new data frames for training (80% of the data) and testing (20%).

"""

# code here and in additional cells if needed
len(clean_igr_df_2)
clean_igr_df_test = clean_igr_df_2.sample(frac=.2)
clean_igr_df_train = clean_igr_df_2.drop(clean_igr_df_test.index)
print(clean_igr_df_test.head(50), clean_igr_df_train.head(50))
#pd.merge(clean_igr_df_test, clean_igr_df_train, how='outer').drop_duplicates()

"""#### Question 2.8 (kNN regression for GBA4RTT, 10 points)

We would like to apply a kNN algorithm that predicts the percentage of
students graduating in 4 years (GBA4RTT).  

Recall that you have to choose: (i) the type of features (categorical, quantitative); (ii) the scaling of the features; (iii) the distance you use; (iv) the features in the model; (v) the number of neighbors, k.

Apply knn regression and predict the graudation rates in the test set.  You can choose to use all the variables or a subset of them.  Report the square root of the mean squared errror (  sqrt( sum ( (ypred - ytest)**2 ) ) )  (RMS accuracy).  

**Note:** You need to write the code for the algorithm and not use kNN functions in Python libraries. You may use any of the functions we wrote in the lecture notebooks.

"""

# They are modified here so they are more general
# Take as input X_train (data frame with attribute data for the training set),
# Y_train (class for training data,0-1)
# X_test and k

# the Euclidean distance
def distance(pt1, pt2):
    """Return the distance between two points, represented as arrays"""
    return np.sqrt(sum((pt1 - pt2)**2))

def all_distances(X_train, point):
    """The distance between point (an array of numbers) and the numbers
    in row i of attribute data frame."""
    attributes = X_train.copy()
    def distance_from_point(row):
        return distance(point, np.array(row))
    return attributes.apply(distance_from_point,axis=1)

def closest(X_train, Y_train, point, k):
    """A df containing the k closest rows in the training df to
    array 'point' """
    attributes=X_train.copy()
    attributes['Distance']=all_distances(attributes, point)
    attributes['Class']=Y_train
    attributes=attributes.sort_values('Distance')
    topk = attributes.head(k)
    return topk

def majority(topkclasses):
    """1 if the majority of the "Class" column is 1's, and 0 otherwise."""
    ones = topkclasses[topkclasses.Class==1].shape[0]
    zeros = topkclasses[topkclasses.Class==0].shape[0]
    if ones > zeros:
        return 1
    else:
        return 0

# returns a data frame with X_test appended by a column of Class prediction
def classify(X_train,Y_train, X_test, k):
    """Classify data in X_test using k-nearest neighbor classification with the given training table."""
    #training = X_train
    n=X_test.shape[0] # the number of test cases

    pred_class = np.array([])
    for i in np.arange(n):
        topkclasses = closest(X_train,Y_train, X_test.iloc[i], k)
        pred_class = np.append(pred_class,majority(topkclasses))

    report_df=X_test.copy()
    report_df['Class']=pred_class

    return report_df   # Y_test

igr_df['GBA4RTT']

#GBA4RTT_2=clean_igr_df_1['GBA4RTT']
#GBA4RTT_2
X_train=clean_igr_df_train.drop(columns=['GBA4RTT'])
X_test=clean_igr_df_test.drop(columns=['GBA4RTT'])
Y_train=clean_igr_df_train.GBA4RTT
Y_test=clean_igr_df_test.GBA4RTT



res_df=classify(X_train,Y_train, X_test, 5)

np.mean(Y_test==res_df.Class)

"""*Write your answer here, replacing this text.*

#### Question 2.9 (kNN classification for top half/bottom half, 6 points)

Write functions to use kNN to predict whether the college (row) is above or below the median in graduation rates.

This is an example of **binary classification**; by construction using the meidan, these classes are balanced.

Issues to consider:
- as before, you need training and testing data
- try several values of k
- feature selection, scaling, distance are important
- how do you break ties?

**Hints (on tie breaking)**: (i) random selection between tied labels; (ii) use the distances when tied (closer neighbors are more relevant).
"""

clean_igr_df_train

# code here and in additional cells if needed



X1_train=clean_igr_df_train.drop(columns=['tophalf'])
X1_test=clean_igr_df_test.drop(columns=['tophalf'])
Y1_train=clean_igr_df_train.tophalf
Y1_test=clean_igr_df_test.tophalf

res1_df=classify(X1_train,Y1_train, X1_test, 1)
res1_df



"""*Write your answer here, replacing this text.

#### Question 2.10 (confusion matrix, 3 points)
Report the confusion matrix of your knn classifier predicting top half/ bottom half.
"""

#confusion matrix
#from sklearn.metrics import confusion_matrix
#cm= confusion_matrix(y_true, y_pred)

#import seaborn as sns
#f, ax =plt.subplots(figsize = (5,5))

#sns.heatmap(cm,annot = True, linewidths= 0.5, linecolor="red", fmt=".0f", ax=ax)
#plt.xlabel("y_pred")
#plt.ylabel("y_true")
#plt.show()



"""# Homework 6: Clustering (50 points)


Please complete this notebook by filling in the cells provided. Before you begin, execute the following cell to load the needed functions. Each time you start your server, you will need to execute this cell again to load them.  

Homework 6 is due on **Friday, February 24 at 11:59pm**. Late work will not be accepted.  
"""

# Commented out IPython magic to ensure Python compatibility.
# Don't change this cell; just run it.
import pandas as pd
import numpy as np
import matplotlib
# %matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

"""## 1. Clustering: Mall Customers (15 points)

Examine the (apparently artificial) dataset "Mall Customer Segementation Data" by kaggle user Vijay Choudhary:
https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python
It presents 200 rows with 4 observations each.  


"""

Mall = pd.read_csv("/Mall_Customers.csv")
Mall.head(50)

"""#### Question 1.1 ( Complete linkage, 4 points)

Using hierarchical clustering with complete linkage and Euclidean distance, cluster the customers. Cut the dendogram at a height that results in three distinct clusters.
"""



def initialize_centroids(df,K):     # What does "random" mean here?
    random_ids = np.random.permutation(df.shape[0])
    centroids = df.iloc[random_ids[:K]]
    return centroids

n_Mall=Mall.drop(columns=['Gender', 'Age', 'CustomerID'])
initialize_centroids(n_Mall, 3)
cent0=initialize_centroids(n_Mall,3)

cent0

# code here and in additional cells if needed

def distance(pt1, pt2):
    """Return the distance between two points, represented as arrays"""
    return np.sqrt(sum((pt1 - pt2)**2))


# calculate distances to centroids
def compute_distance(df, centroids):
    K=centroids.shape[0]
    distances_ar = np.zeros((df.shape[0], K))

    # we used a similar approach  in developing kNN

    for k in range(K):
        point=centroids.iloc[k]
        def distance_from_point(row):
            return distance(point, np.array(row))
        distances_ar[:,k] = df.apply(distance_from_point,axis=1).values
    return distances_ar


dist_matrix=compute_distance(n_Mall, cent0)
# distances for the first 5 points
dist_matrix[:5,]
plt.hist(dist_matrix.reshape((-1,)), bins=100)

clust1 = linkage(dist_matrix, 'average')

plt.figure(figsize=(10, 7))
dendrogram(clust1,
            orientation='top',
            labels=range(0,200),
            distance_sort='descending')
plt.show()
#not this one

def compute_sse(df, labels, centroids,K):
    distances_ar = np.zeros(df.shape[0])
    for k in range(K):
        point=centroids.iloc[k]
        def distance_from_point(row):
            return distance(point, np.array(row))
        distances_ar[labels == k] = df[labels == k].apply(distance_from_point,axis=1).values
    return np.sum(distances_ar)


def compute_centroids(df, labels, K):
        centroids = np.zeros((K, df.shape[1]))
        for k in range(K):
            centroids[k, :] = df[labels == k].mean()
        return centroids


def Kmeans_sse(df,K):
    #define the maximum number of iterations
    max_iter=20

    #initialize centroids
    centroids=initialize_centroids(df,K)

    for i in range(max_iter):
            old_centroids = centroids
            dist_matrix = compute_distance(df, old_centroids)
            clust=np.argmin(dist_matrix, axis=1)
            #print(compute_sse(df,clust,old_centroids,K))
            centroids = pd.DataFrame(compute_centroids(df,clust,K))

    # return SSE
    return compute_sse(df,clust,centroids,K)



clust3 = linkage(Mall.drop(columns=['Gender']), 'average')

LabelList = np.array(Mall.Gender)

plt.figure(figsize=(30, 12))
dendrogram(clust3,
            orientation='top',
            labels=LabelList,
            distance_sort='descending',
            leaf_font_size=9)
' '

"""*Write your answer here, replacing this text.*

#### Question 1.2 (Standard units, 3 points)

Repeat the hierarchical clustering in question 1.1 **after standardizing the four variables.** What effect does standardization have on the clustering obtained? Should the variables be standardized before the clustering? Justify your answer.
"""

# code here and in additional cells if needed
def standard_units(x):
    "Convert any array of numbers to standard units."
    return (x - np.average(x))/np.std(x,ddof=1)

Malls=Mall.rename(columns ={'Annual Income (k$)':'Annual_Income',
                      'Spending Score (1-100)':'Spending_Score'})
Mallsu=pd.DataFrame(
        {"M.CID": standard_units(Malls.CustomerID),
         "M.Age": standard_units(Malls.Age),
        "M.AI": standard_units(Malls.Annual_Income),
        "M.SS": standard_units(Malls.Spending_Score), "Gender" : Malls.Gender})

clust4 = linkage(Mallsu.drop(columns=['Gender']), 'average')

LabelList = np.array(Mallsu.Gender)

plt.figure(figsize=(30, 12))
dendrogram(clust4,
            orientation='top',
            labels=LabelList,
            distance_sort='descending',
            leaf_font_size=9)
plt.show()
Mallsu

Mall.columns

"""What effect does standardization have on the clustering obtained? Should the variables be standardized before the clustering? Justify your answer.

 The variables should be standardized before clustering because standardization makes the clusters more accurate and less likely to vary based on outliers.

#### Question 1.3 (K-means, 4 points)

Perform K-means on the standardized data. For K=1,2,...,10 calculate the objective function (sum of squared deviations).  Plot this **elbow plot** of unexplained residuals vs. k.   Do any values of k jump out?
"""

# code here and in additional cells if needed
K=np.arange(1,11)
sse=np.zeros(10)
for k in K:
    sse[k-1]=Kmeans_sse(Mallsu.drop(columns=['Gender']),k)

plt.plot(K,sse)
plt.xlabel('Number of clusters k')
plt.ylabel('Sum of squared distance');

"""Do any values of k jump out?

The values of 7 and 4 struck out to me.

#### Question 1.4 (K-means clusters, 4 points)

Choose a value of k and perform k-means clustering.   Plot the three scatterplots of the three continuous variables against each other with dots colored according to the class label.  Do any of the plots make you think, "this data looks artificial" ?
"""

def Kmeans(df,K):
    #define the maximum number of iterations
    max_iter=20

    #initialize centroids
    centroids=initialize_centroids(df,K)

    for i in range(max_iter):
            old_centroids = centroids
            dist_matrix = compute_distance(df, old_centroids)
            clust=np.argmin(dist_matrix, axis=1)
#           print(compute_sse(df,clust,old_centroids,K))
            centroids = pd.DataFrame(compute_centroids(df,clust,K))

    # return the centroids
    return centroids

centr=Kmeans(Mallsu.drop(columns=['Gender']),3)
dist_matrix=compute_distance(Mallsu.drop(columns=['Gender']), centr)
cluster=np.argmin(dist_matrix, axis=1)

fig, ax = plt.subplots(figsize=(6, 6))
plt.scatter(Malls[cluster == 0].Age, Malls[cluster == 0].Annual_Income,
            color='green', label='cluster 1')
plt.scatter(Malls[cluster == 1].Age, Malls[cluster == 1].Annual_Income,
            color='green', label='cluster 1')
plt.scatter(Malls[cluster == 2].Age, Malls[cluster == 2].Annual_Income,
            color='green', label='cluster 1')
plt.plot(Malls[cluster == 0].Spending_Score, Malls[cluster == 0].Annual_Income,'ro',
            color='blue', label='cluster 2')
plt.plot(Malls[cluster == 1].Spending_Score, Malls[cluster == 1].Annual_Income,'ro',
            color='blue', label='cluster 2')
plt.plot(Malls[cluster == 2].Spending_Score, Malls[cluster == 2].Annual_Income,'ro',
            color='blue', label='cluster 2')
plt.plot(Malls[cluster == 0].Age, Malls[cluster == 0].Spending_Score,'ro',
            color='orange', label='cluster 3')
plt.plot(Malls[cluster == 1].Age, Malls[cluster == 1].Spending_Score,'ro',
            color='orange', label='cluster 3')
plt.plot(Malls[cluster == 2].Age, Malls[cluster == 2].Spending_Score,'ro',
            color='orange', label='cluster 3')


plt.legend()
plt.xlim([-20,150])
plt.ylim([-10, 150])

"""Do any of the plots make you think, "this data looks artificial" ?

  The shape for the plots was very comparable which did make me thing that some of the data was skewed or chosen with this shape in mind.

## 2. State voting (28 points)

We look here at how states voted in presidential elections between 1972 and 2020. We would like to identify clusters showing groups of states that vote similarly.

#### Question 2.1 ( Obtaining data , 4 points)

We are asking you to get the data yourself. Specifically, we'd like you to use this table from wikipedia: https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state.

You can convert the table into csv format using this website: https://wikitable2csv.ggor.de/. Simply paste the URL of the article into wikitable2csv and leave the default options as they are. Then click download on Table 1, and you should download a file called table-1.csv.

Move this file in your working folder and rename it presidential_elections.csv. Then run the cells below to make sure that you did everything properly.
"""

#from Data119 ed discussion
from bs4 import BeautifulSoup
import requests

url = "https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state"
soup = BeautifulSoup(requests.get(url).text, "html.parser")
table = soup.find("table", {"class": "wikitable"})
df = pd.read_html(str(table))[0]
df

#df = pd.read_csv("/presidential_elections.csv")
#df.shape

df.head(5)

"""#### Question 2.2 ( Data cleaning, 4 points)

The data in this table is messy. Create a clean version of this table called `df_1972_2020_clean`. It should contain exactly 51 rows (corresponding to the 50 states plus Washington DC) and 13 columns (one for each of the election years from 1972 to 2020).

Also create an array with the state names, `states`, that has length 51 and contains the states in the order corresponding to the rows of the data frame.

Hint. The following panda functions are useful: iloc, drop, rename.

**Notes**:
- Feel free to open your csv file in Excel or Google Sheets to explore the data if you find that easier. However, we require that you do your actual data cleaning in pandas, i.e. don't just delete and rename columns in Excel.
- In your projects, it is sometimes more convenient to manually do your data cleaning using Excel or other tools. The downside of doing this is that you have no record of what you did, and if you have to redownload the data, you have to redo the manual data cleaning process.
"""

from pandas.core.arrays.interval import NA
# code here and in additional cells if needed
#df[['1972', '1976', '1980', '1984','1988', '1992', '1996', '2000\xa0â¡',
#    '2004', '2008', '2012', '2016\xa0â¡', '2020']]
df.iloc[0] == df.iloc[53]
df.head(53)
df1=df.iloc[:, 52:67].drop(columns=[60])
(df1.iloc[0].dropna() == df1.iloc[26].dropna()).value_counts()
hi = df1.iloc[26]
df1.columns = hi
df2 = df1.iloc[1:53]
df3 =df2.sort_values('2020').reset_index().drop(columns=['index']).iloc[1:52].sort_values(
    "State").reset_index().drop(columns=['index'])
#df2.iloc[1:53].dropna()
df3.index = df3.loc[:, 'State']

clean_df = df3.iloc[:, 0:13]

state_array = pd.array(df3.index, dtype=str)
clean_df

"""#### Question 2.3 ( Convert to indicator variable, 2 points)

To perform clustering, we need to convert our data into being numerical. To do this, replace all of the "D" characters with the number 0, and all of the "R" characters with the number 1. Assign the resulting table to `df_1972_2020_numerical`.
"""

# code here and in additional cells if needed
diction = {"R": 1, "D":0}
numerical_df = clean_df.replace(diction)
numerical_df

"""#### Question 2.4 (Similar states , 5 points)

As you might expect, some states voted exactly alike in these elections. This question asks you to find:
1. The list of states that voted only Republican over this time period.
2. The list of states that voted only Democratic over this time period.
3. The list of states that voted exactly the same as Illinois over this time period.

Comment briefly on your findings (were you surprised by the IL list?).

"""

# code here and in additional cells if needed
numerical_df.value_counts()
numerical_df.sum(axis=1)
numerical_df
Rep = numerical_df.loc[numerical_df.sum(axis=1) == 13,:]
Dem = numerical_df.loc[numerical_df.sum(axis=1) == 0,:]
print('The states that voted only republican during this time period:',
      Rep.index,
      '\nThe states that voted only democratic during this time period:',
      Dem.index)

Try_df = numerical_df == numerical_df.iloc[13]
Try_df.all(True)
IL = numerical_df.loc[Try_df.all(True) == True,:]
IL

print('The states that voted only republican during this time period:',
      Rep.index,
      '\nThe states that voted only democratic during this time period:',
      Dem.index,
      '\nThe states that voted the same as Illinois during this time period:',
      IL.index)

"""The states that voted only republican during this time period:
'Alaska', 'Idaho', 'Kansas', 'Nebraska', 'North Dakota', 'Oklahoma', 'South Dakota', 'Utah', and 'Wyoming'

The states that voted only democratic during this time period: 'D.C'

The states that voted the same as Illinois during this time period: 'California', 'Connecticut', 'Maine', 'New Jersey', 'Vermont'

Overall I wasn't extremely surprised with the results especially not the states that voted the same as Illinois. The only point where I was slightly shocked with these results was with Vermont being listed as having a similar result, but that largely stems from my lack of knowledge regarding Vermont.

#### Question 2.5 ( State clustering, 10 points)

You are asked here to cluster states according to their voting record and write a short description of your findings.

You need to decide on the method of clustering and on the best way to present your results.
"""

# code here and in additional cells if needed
clean_df
nnumerical_df = numerical_df.sum(axis=1)
import seaborn as sns
numerical_df
#numerical_df.iloc[2].cumsum()
numerical_df.cumsum(axis=1)
sns.clustermap(numerical_df.cumsum(axis=1), yticklabels=True)
plt.title("Cumulative voting records for each US state per year")
numerical_df.cumsum(axis=1)

"""This map depicts the sumulative sum of each of the states voting records, which I thought was the best way to present the results. There was a noticeable degree of variation between states that were consistent in voting records and those who were more likely to switch parties which further research could be done on. As the years progress the variation between states also increasingly differs with very few maintaining the same color throughout.

#### Question 3.1  Data Cleaning (5 points)
The Center for Near Earth Object Studies has collected a database of large, bright objects that have impacted the Earth and were detected by the flashes they made, day or night, in the atmosphere https://cneos.jpl.nasa.gov/fireballs/.  This database has about 900 events spread out over the past 30 years.

Clean up the column for latitude in python and plot a histogram of fireballs by longitude.  (What is the right domain for longitude?)

Do the fireballs look uniform or not by latitude? They look uniform

**Hint** Write a function that takes a string like "134.54W", breaks it into two parts, and returns a different number depending on the hemisphere sign E or W.  df.apply() or map() can apply that function to all the data in a dataframe and get you the numbers you need.
"""

cneos_df = pd.read_csv("/cneos_fireball_data.csv")
cneos_df.head(50)

cneos_df['Longitude_dir'] = cneos_df.loc[:, 'Longitude (deg.)']
cneos_df['Latitude_dir'] = cneos_df.loc[:, 'Latitude (deg.)']
cneos_df['Latitude (deg.)']=cneos_df['Latitude (deg.)'].str.replace('N', '')
cneos_df['Latitude (deg.)']=cneos_df['Latitude (deg.)'].str.replace('S', '')
cneos_df['Longitude (deg.)']=cneos_df['Longitude (deg.)'].str.replace('E', '')
cneos_df['Longitude (deg.)']=cneos_df['Longitude (deg.)'].str.replace('W', '')

cneos_df['Latitude (deg.)'] = pd.to_numeric(cneos_df['Latitude (deg.)'])
cneos_df['Longitude (deg.)'] = pd.to_numeric(cneos_df['Longitude (deg.)'])

def new_cneos_df(hello, goodbye, look):
    hello.loc[hello[goodbye].str.contains(look) == True, goodbye] = look
    return hello
new_cneos_df(cneos_df, 'Longitude_dir', 'E')
new_cneos_df(cneos_df, 'Longitude_dir', 'W')
new_cneos_df(cneos_df, 'Latitude_dir', 'N')
new_cneos_df(cneos_df, 'Latitude_dir', 'S')

cneos_df.loc[cneos_df['Longitude_dir'].str.contains('W') == True,
             'Longitude (deg.)'] = -1*cneos_df.loc[cneos_df['Longitude_dir'] ==
                                                   'W' , 'Longitude (deg.)']

cneos_df.loc[cneos_df['Latitude_dir'].str.contains('S') == True,
             'Latitude (deg.)'] = -1*cneos_df.loc[cneos_df['Latitude_dir'] ==
                                                   'S' , 'Latitude (deg.)']


cneos_df
cneos_df[['Longitude (deg.)', 'Latitude (deg.)']].plot.hist()